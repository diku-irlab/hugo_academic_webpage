+++
date=2019-05-02
title="Two papers accepted at WWW workshops"
+++

The IR lab got papers accepted at two different WWW workshops: <i>International Workshop on Misinformation, Computational Fact-Checking and Credible Web</i> and <i>International Workshop of Deep Learning for Graphs and Structured Data Embedding</i>


<a href={{<ref "publication/hhwww-19/index.md">}}><h3>Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences for Fact-Checking
</h3></a>
Automatic fact-checking systems detect misinformation, such as
fake news, by (i) selecting check-worthy sentences for fact-checking,
(ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences. Most prior research on (i) uses
hand-crafted features to select check-worthy sentences, and does
not explicitly account for the recent finding that the top weighted
terms in both check-worthy and non-check-worthy sentences are
actually overlapping [15]. Motivated by this, we present a neural
check-worthiness sentence ranking model that represents each
word in a sentence by both its embedding (aiming to capture its
semantics) and its syntactic dependencies (aiming to capture its
role in modifying the semantics of other terms in the sentence).
Our model is an end-to-end trainable neural network for checkworthiness ranking, which is trained on large amounts of unlabelled
data through weak supervision. Thorough experimental evaluation
against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP
and +28% at various Precision cut-offs from the best baseline with
statistical significance). Empirical analysis of the use of weak supervision, word embedding pretraining on domain-specific data,
and the use of syntactic dependencies of our model reveals that
check-worthy sentences contain notably more identical syntactic
dependencies than non-check-worthy sentences

<a href={{<ref "publication/wang-lcsl-19/index.md">}}><h3>Contextual Compositionality Detection with External Knowledge Basesand Word Embeddings</h3></a>
When the meaning of a phrase cannot be inferred from the individual meanings of its words (e.g., hot dog), that phrase is said to be
non-compositional. Automatic compositionality detection in multiword phrases is critical in any application of semantic processing,
such as search engines [9]; failing to detect non-compositional
phrases can hurt system effectiveness notably. Existing research
treats phrases as either compositional or non-compositional in a
deterministic manner. In this paper, we operationalize the viewpoint that compositionality is contextual rather than deterministic,
i.e., that whether a phrase is compositional or non-compositional
depends on its context. For example, the phrase “green card” is
compositional when referring to a green colored card, whereas it is
non-compositional when meaning permanent residence authorization. We address the challenge of detecting this type of contextual
compositionality as follows: given a multi-word phrase, we enrich the word embedding representing its semantics with evidence
about its global context (terms it often collocates with) as well as
its local context (narratives where that phrase is used, which we
call usage scenarios). We further extend this representation with information extracted from external knowledge bases. The resulting
representation incorporates both localized context and more general usage of the phrase and allows to detect its compositionality in
a non-deterministic and contextual way. Empirical evaluation of our
model on a dataset of phrase compositionality1
, manually collected
by crowdsourcing contextual compositionality assessments, shows
that our model outperforms state-of-the-art baselines notably on
detecting phrase compositionality.

